<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="Since OpenAI announced its text-to-video (T2V) tool Sora this week, manic predictions have ensued with varying combinations of awe and trepidation, acclaim and dismay, over what appeared to be a massive leap forward in T2V capability compared with similar publicly available tools, such as Runways Gen-2 and Pika."><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>Why OpenAIs Sora Isnt Ready to Replace Hollywood</title><link rel=canonical href=./why-openai-sora-wont-replace-hollywood-1235914294.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>SnogBlogy</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>Why OpenAIs Sora Isnt Ready to Replace Hollywood</h1><div id=sub-header>July 2024 · 5 minute read</div><div class=entry-content><img src=https://cdn.statically.io/img/variety.com/wp-content/uploads/2024/02/featured_ai_movie_camera_r1.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Since OpenAI announced its text-to-video (T2V) tool <a rel="noreferrer noopener nofollow" href=#>Sora</a> this week, manic predictions have ensued with varying combinations of awe and trepidation, acclaim and dismay, over what appeared to be a massive leap forward in T2V capability compared with similar publicly available tools, such as Runway’s <a rel="noreferrer noopener nofollow" href=#>Gen-2</a> and <a rel="noreferrer noopener nofollow" href=#>Pika</a>.</p><figure class="wp-block-embed is-type-rich is-provider-datawrapper wp-block-embed-datawrapper"></figure><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Judging purely by its sample outputs, OpenAI’s Sora is the most impressive video diffusion model to date. But Sora and other video diffusion models are still limited in ways that would make them inept for Hollywood filmmaking.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">“Sora is a tremendous achievement in the direction of realistic content that will be useful in high-end entertainment. Today, creatives demand total control over performances and what is in a scene, so there is still a long way to go before diffusion models can generate Hollywood movies,” Tom Graham, CEO and cofounder of AI firm <a rel="noreferrer noopener nofollow" href=#>Metaphysic</a> responsible for deaging Tom Hanks in the Miramax film "Here," told VIP+.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">But first, what makes Sora a leap forward exactly?</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">The model’s capabilities are identical to those already offered by other video diffusion models from Runway and Pika, including video generation and video editing. These include generating a novel short video based on a text prompt, generating a video based on a 2D image (e.g., animating an image), and inpainting (replacing or inserting new visual elements) and outpainting (extending a shot beyond its original frame, filling in with context-relevant content).</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">But Sora improves on or newly achieves a few things:</p><ul><li><strong>Video quality and realism:</strong> Most evidently, its videos appear significantly more photorealistic — with higher fidelity — compared to outputs from other models.</li><li><strong>Video length:</strong> Sora’s video outputs can be up to a minute long while maintaining coherence to the prompt, significantly longer than Runway’s Gen-2, which can generate up to 18 seconds per generation as of its August 2023 update, up from just four.</li><li><strong>Spatiotemporal consistency<strong>:</strong> </strong>Sora also promises to be able to extend generated videos to make them longer. But the power of this capability is best understood in the context of another: that by giving the model “foresight” of many frames at a time, Sora solves the problem of “making sure a subject stays the same even when it goes out of view temporarily.”</li></ul><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">This conceivably resolves a pain point for those who would try to create an AI-generated film that stitches together multiple video outputs as “shots.” Such attempts struggle to maintain character and scene continuity because repeated generation using the same prompt wording or conditioning parameters will never result in the model producing an identical result. This “extender” capability that can maintain character or object continuity from one output to the next could enable longer AI-generated storytelling from video diffusion tools.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Yet despite these advances, the following still present real barriers for models like Sora to be used in Hollywood productions:</p><ul type=1><li><strong>Continuity:</strong> Sora’s promised improvements aren’t total guarantees of subject/object and environment continuity enough to ensure a coherent narrative or look of a film or TV show. Like other models, too, Sora also isn’t free of occasionally misconstruing the way the real world looks or behaves, with “physics fails” that have been observed in outputs from other image and video models.</li><li><strong>Controllability:</strong> Some have analogized these models to a camera, albeit where video is rendered rather than physically recorded. But as I’ve previously discussed, so far, these tools don’t offer filmmakers sufficient <a rel="noreferrer noopener" href=#>creative control</a> and precision to derive and manipulate their outputs —&nbsp;meaning AI could prove to be more, not less, difficult and constraining than traditional methods in the near term. That’s changing bit by bit with new control parameters being added to software, but it doesn’t automatically mean AI video vastly improves over camera footage.</li><li><strong>Copyright:</strong> More consequentially, Hollywood productions are highly unlikely to use these outputs for on-screen footage without more clarity on all sides of copyright law and generative AI. Significant <a rel="noreferrer noopener" href=#>questions remain open</a>, including whether AI-assisted works from such models will be copyright protectable and whether AI-generated material is an infringement liability due to the <a rel="noreferrer noopener nofollow" href=#>strong likelihood</a> that models have trained on copyrighted material.</li></ul><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">As I argued in our December 2023 special report, “<a rel="noreferrer noopener" href=#>Generative AI in Film & TV</a>, “At least in the near term, until legal questions are more clearly resolved, studio decisions about using AI-generated images or videos in production remain problematic and will prevent the use of generative AI tools for production assets that will show up on screen.”</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">As a result of these constraints, in the near term, these tools are most likely to materialize during previsualization stages of a project, such as to rapidly develop and iterate concept art, character design or animatics.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">But even early-stage concept work is potentially not failsafe against infringement claims or questions of protectability if, for example, a studio, creative team or artist generates an interesting character or environment that’s then used in a human-created TV, movie or video game.</p><figure class="wp-block-embed is-type-rich is-provider-datawrapper wp-block-embed-datawrapper"></figure><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Sora isn’t publicly available (yet) and will be red-teamed to determine vulnerabilities or vectors of misuse. OpenAI also pledged to get feedback from policymakers, educators and artists around the world to understand concerns and identify beneficial use cases.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">That’s similar to the conclusion reached by Google researchers who presented <a rel="noreferrer noopener nofollow" href=#>Lumiere</a>, a new T2V diffusion model, in late January, saying while they believed the tool offered creative possibilities, “there is a risk of misuse for creating fake or harmful content with our technology.”</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m">Assuming that Sora gets its public release, Sora stands to become a tool for social media creators and average users to flex their creativity, instantly flooding platforms with generated video. Advertisers and content marketers may also manage to use it. Unfortunately, deepfake disinformation is another likely outcome, regardless of watermarks. The verdict of OpenAI’s conversations remains to be seen.</p><p class="paragraph larva // lrv-u-margin-lr-auto lrv-a-font-body-m lrv-u-text-align-center"><strong><a href=# rel="noreferrer noopener">VIP+ Analysis: Gen AI Explored From All Angles — Pick a Story</a></strong></p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmiukae2psDYZ5qopV%2BrtrF71qGwZqegmruitYyspquZXay8r8CMq5yppJGYsm60zqWjsq%2BfpLFufZFsbHJpZGeGdXs%3D</p></div><div id=links><a href=./28-years-later-release-date-june-2025-1235999311.html>&#171;&nbsp;28 Years Later Gets June 2025 Release Date</a>
<a href=./veteran-nollywood-actor-don-brymo-uchegbu-is-dead.html>Veteran Nollywood actor Don Brymo Uchegbu is dead&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>